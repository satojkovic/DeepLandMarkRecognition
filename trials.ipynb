{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import requests\n",
    "import shutil\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('google-landmarks-dataset/train.csv')\n",
    "test_data = pd.read_csv('google-landmarks-dataset/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "landmark_list = [str(x) for x in range(1000, 3001)]\n",
    "train_data_sample = train_data[train_data['landmark_id'].isin(landmark_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('sample train data:', len(train_data_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = np.array(['#4285f4','#34a853','#fbbc05','#ea4335'])\n",
    "#Define the order in which to display the graph\n",
    "order = ['1-5','5-10','10-50','50-100','100-200','200-500','>=500']\n",
    "f, (ax1, ax2) = plt.subplots(1, 2,figsize=(15,5))\n",
    "\n",
    "def plot_distribution(data_f, data_k, axis):\n",
    "    x = data_f.landmark_id.value_counts().index\n",
    "    y = pd.DataFrame(data_f.landmark_id.value_counts())\n",
    "    \n",
    "    y['Number of images'] = np.where(y.landmark_id >= 500, '>=500', y['landmark_id'])\n",
    "    y['Number of images'] = np.where((y.landmark_id >= 200) & (y.landmark_id < 500), '200-500', y['Number of images'])\n",
    "    y['Number of images'] = np.where((y.landmark_id >= 100) & (y.landmark_id < 200), '100-200', y['Number of images'])\n",
    "    y['Number of images'] = np.where((y.landmark_id >= 50) & (y.landmark_id < 100), '50-100', y['Number of images'])\n",
    "    y['Number of images'] = np.where((y.landmark_id >= 10) & (y.landmark_id < 50), '10-50', y['Number of images'])\n",
    "    y['Number of images'] = np.where((y.landmark_id >= 5) & (y.landmark_id < 10), '5-10', y['Number of images'])\n",
    "    y['Number of images'] = np.where((y.landmark_id >= 1) & (y.landmark_id < 5), '1-5', y['Number of images'])\n",
    "\n",
    "    y['Number of images'].value_counts().loc[order].plot(kind = 'bar',color = colors,width = 0.8, ax=axis)\n",
    "    axis.set_xlabel('Number of images')\n",
    "    axis.set_ylabel('Number of classes')\n",
    "    axis.set_title(data_k)\n",
    "\n",
    "plot_distribution(train_data, 'Original', ax1)\n",
    "plot_distribution(train_data_sample, 'Sample', ax2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rewrite urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_SIZE = 96\n",
    "def reso_overwrite(url_tail, resolution=TARGET_SIZE):\n",
    "    pattern = 's[0-9]+'\n",
    "    matched = re.match(pattern, url_tail)\n",
    "    if matched:\n",
    "        return 's{}'.format(resolution)\n",
    "    else:\n",
    "        return url_tail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_url(parsed_url, s_reso):\n",
    "    parsed_url[-2] = s_reso\n",
    "    return '/'.join(parsed_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overwrite_urls(df):\n",
    "    df = df[df.url.apply(lambda x: len(x.split('/')) > 1)]\n",
    "    parsed_url = df.url.apply(lambda x: x.split('/'))\n",
    "    url_tail = parsed_url.apply(lambda x: x[-2])\n",
    "    resos = url_tail.apply(lambda x: reso_overwrite(x, TARGET_SIZE))\n",
    "    overwritten_df = pd.concat([parsed_url, resos], axis=1)\n",
    "    overwritten_df.columns = ['url', 's_reso']\n",
    "    df['url'] = overwritten_df.apply(lambda x: join_url(x['url'], x['s_reso']), axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_sample_resize = overwrite_urls(train_data_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_sample_resize.url.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split train, test and validation data from train_data_sample_resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# to check\n",
    "#\n",
    "sample_randmak_id = train_data_sample_resize.landmark_id.iloc[0]\n",
    "sample_li = train_data_sample_resize[train_data_sample_resize.landmark_id == sample_randmak_id]\n",
    "print(len(sample_li))\n",
    "\n",
    "# select test set\n",
    "sample_li_test = sample_li.sample(frac=ratio_test)\n",
    "print(len(sample_li_test))\n",
    "sample_li = sample_li[~sample_li.id.isin(sample_li_test.id)]\n",
    "print(len(sample_li))\n",
    "\n",
    "# select valid set\n",
    "sample_li_valid = sample_li.sample(frac=ratio_valid)\n",
    "print(len(sample_li_valid))\n",
    "sample_li = sample_li[~sample_li.id.isin(sample_li_valid.id)]\n",
    "print(len(sample_li))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_train = pd.DataFrame(columns=['id', 'url', 'landmark_id'])\n",
    "train_test = pd.DataFrame(columns=['id', 'url', 'landmark_id'])\n",
    "train_valid = pd.DataFrame(columns=['id', 'url', 'landmark_id'])\n",
    "ratio_test = 0.1\n",
    "ratio_valid = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for landmark_id in set(train_data_sample_resize['landmark_id']):\n",
    "    # get list for each landmark_id\n",
    "    li = train_data_sample_resize[train_data_sample_resize.landmark_id == landmark_id]\n",
    "    # select test set\n",
    "    li_test = li.sample(frac=ratio_test)\n",
    "    li = li[~li.id.isin(li_test.id)]\n",
    "    # select valid set\n",
    "    li_valid = li.sample(frac=ratio_valid)\n",
    "    li = li[~li.id.isin(li_valid.id)]\n",
    "    \n",
    "    train_train = train_train.append(li)    \n",
    "    train_test = train_test.append(li_test)\n",
    "    train_valid = train_valid.append(li_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_train), len(train_test), len(train_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dir(dir_path):\n",
    "    if not os.path.exists(dir_path):\n",
    "        print('Created: {}'.format(dir_path))\n",
    "        os.makedirs(dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_train_images_dir = 'train_train_images'\n",
    "train_valid_images_dir = 'train_valid_images'\n",
    "train_test_images_dir = 'train_test_images'\n",
    "create_dir(train_train_images_dir)\n",
    "create_dir(train_valid_images_dir)\n",
    "create_dir(train_test_images_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_image(url, dir_path):\n",
    "    # fetch image and save as test.jpg(dummy name)\n",
    "    response = requests.get(url, stream=True)\n",
    "    dummy = os.path.join(dir_path, 'test.jpg')\n",
    "    with open(dummy, 'wb') as f:\n",
    "        response.raw.decode_content = True\n",
    "        shutil.copyfileobj(response.raw, f)\n",
    "    return dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_images(data, dir_path):\n",
    "    idx = 0\n",
    "    urls = data['url']\n",
    "    for url in tqdm(urls):\n",
    "        # Skip if already fetched\n",
    "        if os.path.exists(os.path.join(dir_path, data['id'].iloc[idx] + '.jpg')):\n",
    "            idx += 1\n",
    "            continue\n",
    "        # fetch image\n",
    "        dummy = fetch_image(url, dir_path)\n",
    "        # rename\n",
    "        os.rename(dummy, os.path.join(dir_path, data['id'].iloc[idx] + '.jpg'))\n",
    "        idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch_images(train_train, train_train_images_dir)\n",
    "fetch_images(train_valid, train_valid_images_dir)\n",
    "fetch_images(train_test, train_test_images_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dirs(dataset, root_dir):\n",
    "    df = pd.DataFrame(dataset.landmark_id.value_counts())\n",
    "    df.reset_index(inplace=True)\n",
    "    df.columns = ['landmark_id', 'count']\n",
    "    num_dirs = 0\n",
    "    for idx, row in dataset.iterrows():\n",
    "        landmark_id = row.landmark_id\n",
    "        landmark_id_dir = os.path.join(root_dir, landmark_id)\n",
    "        if not os.path.exists(landmark_id_dir):\n",
    "            os.makedirs(landmark_id_dir)\n",
    "            num_dirs += 1\n",
    "    print('Created:', num_dirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_dirs(train_test, train_test_images_dir)\n",
    "create_dirs(train_valid, train_valid_images_dir)\n",
    "create_dirs(train_train, train_train_images_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_images(dataset, root_dir):\n",
    "    num_moved = 0\n",
    "    num_removed = 0\n",
    "    for idx, row in dataset.iterrows():\n",
    "        fn = row.id + '.jpg'\n",
    "        from_path = os.path.join(root_dir, fn)\n",
    "        to_path = os.path.join(root_dir, row.landmark_id)\n",
    "        if os.path.getsize(from_path) > 1000:\n",
    "            shutil.move(from_path, to_path)\n",
    "            num_moved += 1\n",
    "        else:\n",
    "            num_removed += 1\n",
    "    print('Moved: {}, Removed {}'.format(num_moved, num_removed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "move_images(train_test, train_test_images_dir)\n",
    "move_images(train_valid, train_valid_images_dir)\n",
    "move_images(train_train, train_train_images_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import preprocess_input, decode_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VGG16(weights='imagenet', include_top=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = train_test.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = image.load_img(os.path.join(train_test_images_dir, row.landmark_id.values[0], row.id.values[0] + '.jpg'), target_size=(224, 224))\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = preprocess_input(x)\n",
    "preds = model.predict(x)\n",
    "print('Predicted:', decode_predictions(preds, top=3)[0])\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
